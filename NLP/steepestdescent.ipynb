{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthode de plus forte pente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons $f \\in C^1$. Le méthode de plus forte pente consiste à calculer itérativement\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha^* \\nabla f(x^k)\n",
    "$$\n",
    "où $\\alpha^* \\in \\arg\\min_{\\alpha \\geq 0} f(x_k - \\alpha \\nabla f(x_k))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons besoin de la librairie `LinearAlgebra` pour accéder à des méthodes comme `det`, qui calcul le déterminant d'une matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons la fonction bivariée\n",
    "$$\n",
    "f(x, y) = 4x^2 - 4xy + 2y^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1(x) = 4x[1]*(x[1]-x[2])+2*x[2]*x[2]\n",
    "\n",
    "default(size=(600,600), fc=:heat)\n",
    "x, y = -2.5:0.1:2.5, 0.5:0.1:2.5\n",
    "z = Surface((x,y)->f1([x,y]), x, y)\n",
    "surface(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Son gradient est\n",
    "$$\n",
    "\\nabla f(x, y) = \\begin{pmatrix} 8x - 4y \\\\ 4y - 4x \\end{pmatrix}\n",
    "$$\n",
    "La matrice hessienne est\n",
    "$$\n",
    "\\nabla f^2(x,y) =\n",
    "\\begin{pmatrix}\n",
    "8 & -4 \\\\ -4 & 4\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [8 -4; -4 4 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les déterminants des mineurs principaux sont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8\n",
    "det( A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dès lors, la matrice est définie positive. Nous pouvons le confirmer en calculant ses valeurs propres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous calculons le gradient comme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f1grad(x)\n",
    "    return [8*x[1]-4*x[2], 4*x[2]-4*x[1]]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons $x_0 = (2, 3)$. Dès lors $\\nabla f(x_0) = (4, 4)$.\n",
    "\n",
    "Nous devons minimiser la fonction univariée\n",
    "$$\n",
    "m(\\alpha) = f((2, 3) - \\alpha(4, 4)) = f(2 - 4\\alpha, 3 - 4\\alpha)\n",
    "$$\n",
    "La dérivée de $m(\\alpha)$ est\n",
    "\\begin{align*}\n",
    "m'(\\alpha) &= \\nabla_{(x,y)} f(2 - 4\\alpha, 3 - 4\\alpha)^T \\nabla_{\\alpha} \\begin{pmatrix} 2 - 4\\alpha \\\\ 3 - 4\\alpha \\end{pmatrix} \\\\\n",
    "&= \\begin{pmatrix} 8(2-4\\alpha) - 4(3-4\\alpha) & 4(3-4\\alpha) - 4(2-4\\alpha)\\end{pmatrix}\\begin{pmatrix} -4 \\\\ -4 \\end{pmatrix} \\\\\n",
    "&= -\\begin{pmatrix} 4 - 16\\alpha & 4\\end{pmatrix}\\begin{pmatrix} 4 \\\\ 4 \\end{pmatrix} \\\\\n",
    "&= -16+64\\alpha-16\\\\\n",
    "&= 64\\alpha-32\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dérivée seconde de $m(\\alpha)$ est\n",
    "$$\n",
    "m''(\\alpha) = 64\n",
    "$$\n",
    "Le modèle unidimensionel est dès lors strictement convexe. Le minimiseur peut être trouvé en posant $m'(\\alpha^*) = 0$, menant à $\\alpha^* = \\frac{1}{2}$. Ainsi,\n",
    "$$\n",
    "x_1 = x_0 - \\frac{1}{2}\\nabla f(x_0) = (2, 3) - \\frac{1}{2}(4, 4) = (0, 1),\n",
    "$$\n",
    "et\n",
    "$$\n",
    "\\nabla f(x_1) = \\begin{pmatrix} -4 \\\\ 4 \\end{pmatrix}\n",
    "$$\n",
    "La fonction univariée à minimiser est à présent\n",
    "$$\n",
    "m(\\alpha) = f((0, 1) - \\alpha(-4, 4)) = f(4\\alpha, 1 - 4\\alpha)\n",
    "$$\n",
    "et sa dérivée est\n",
    "\\begin{align*}\n",
    "m'(\\alpha) &= \\nabla_{(x,y)} f(4\\alpha, 1 - 4\\alpha)^T \\nabla_{\\alpha} \\begin{pmatrix} 4\\alpha \\\\ 1 - 4\\alpha \\end{pmatrix} \\\\\n",
    "&= ( 8 \\times 4\\alpha - 4(1-4\\alpha), 4(1-4\\alpha) - 4\\times(4\\alpha))\\begin{pmatrix} 4 \\\\ -4 \\end{pmatrix} \\\\\n",
    "&= ( -4 + 48\\alpha, 4 - 32 \\alpha)\\begin{pmatrix} 4 \\\\ -4 \\end{pmatrix} \\\\\n",
    "&= -32+320\\alpha\n",
    "\\end{align*}\n",
    "La racine de $m'(\\alpha)$ est $\\alpha^* = \\frac{1}{10}$, et $m''(\\alpha) = 320$, aussi $\\alpha^*$ est un minimiseur global.\n",
    "Nous obtenons\n",
    "$$\n",
    "x_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\frac{1}{10}\\begin{pmatrix} -4 \\\\ 4 \\end{pmatrix}\n",
    "= \\begin{pmatrix} \\frac{4}{10} \\\\ \\frac{6}{10} \\end{pmatrix}\n",
    "= \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{3}{5} \\end{pmatrix}\n",
    "$$\n",
    "Nous pourrions continuer, mais un tel calcul à la main est fastidieux. Nous allons automatiser la procédure en construisant une fonction Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function steepestdescent(f::Function, fprime::Function, x0, h::Float64, verbose::Bool = true,\n",
    "                         record::Bool = false, tol::Float64 = 1e-7, maxiter::Int64 = 1000)\n",
    "\n",
    "    function fsearch(α::Float64)\n",
    "        return(f(x-α*grad))\n",
    "    end\n",
    "\n",
    "    x = x0\n",
    "    k = 0\n",
    "\n",
    "    grad = fprime(x)\n",
    "\n",
    "    if (verbose || record)\n",
    "        fx = f(x)\n",
    "    end\n",
    "    if (verbose)\n",
    "        println(\"$k. x = $x, f($x) = $fx\")\n",
    "    end\n",
    "    if (record)\n",
    "        iterates = [ fx x' ]\n",
    "    end\n",
    "    \n",
    "    while ((k < maxiter) && (norm(grad) > tol))\n",
    "        α = Optim.minimizer(optimize(fsearch, 0, h, GoldenSection()))\n",
    "        x = x-α*grad\n",
    "        k += 1\n",
    "        grad = fprime(x)       \n",
    "        if (verbose || record)\n",
    "            fx = f(x)\n",
    "        end\n",
    "        if (verbose)\n",
    "            println(\"$k. x = $x, f($x) = $fx\")\n",
    "        end\n",
    "        if (record)\n",
    "            iterates = [ iterates; fx x' ]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if (k == maxiter)\n",
    "        println(\"WARNING: maximum number of iterations reached\")\n",
    "    end\n",
    "\n",
    "    if (record)\n",
    "        return x, iterates\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variante suivante propose d'élargir l'intervalle dans lequel a lieu la recherche unidimensionnelle quand la borne supérieur est atteinte.\n",
    "\n",
    "Ce n'est valable que pour des fonctions convexes!\n",
    "\n",
    "L'idée sera généralisée lors de la discussion sur les régions de confiance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function steepestdescent_convex(f::Function, fprime::Function, x0, h::Float64, verbose::Bool = true,\n",
    "        record::Bool = false, tol::Float64 = 1e-7, maxiter::Int64 = 1000)\n",
    "\n",
    "    function fsearch(α::Float64)\n",
    "        return(f(x-α*grad))\n",
    "    end\n",
    "\n",
    "    x = x0\n",
    "    k = 0\n",
    "\n",
    "    grad = fprime(x)\n",
    "\n",
    "    if (verbose || record)\n",
    "        fx = f(x)\n",
    "    end\n",
    "    if (verbose)\n",
    "        println(\"$k. x = $x, f($x) = $fx\")\n",
    "    end\n",
    "    if (record)\n",
    "        iterates = [ fx x' ]\n",
    "    end\n",
    "\n",
    "    Δ = 1e-6\n",
    "    \n",
    "    while ((k < maxiter) && (norm(grad) > tol))\n",
    "        α = Optim.minimizer(optimize(fsearch, 0, h, GoldenSection()))\n",
    "        while ((h-α) <= Δ)\n",
    "            h *= 2\n",
    "            α = Optim.minimizer(optimize(fsearch, α, h, GoldenSection()))\n",
    "        end\n",
    "        h = α\n",
    "        x = x-α*grad\n",
    "        k += 1\n",
    "        grad = fprime(x)       \n",
    "        if (verbose || record)\n",
    "            fx = f(x)\n",
    "        end\n",
    "        if (verbose)\n",
    "            println(\"$k. x = $x, f($x) = $fx\")\n",
    "        end\n",
    "        if (record)\n",
    "            iterates = [ iterates; fx x' ]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if (k == maxiter)\n",
    "        println(\"WARNING: maximum number of iterations reached\")\n",
    "    end\n",
    "\n",
    "    if (record)\n",
    "        return x, iterates\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'exécution de cette fonction donne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent(f1, f1grad, [2.0,3.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent(f1, f1grad, [10.0,10.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent(f1, f1grad, [100.0,100.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous convergeons vers la solution $(0,0)$, mais la méthode est très lente près de la solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent(f1, f1grad, [2.0,3.0], 0.1, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent_convex(f1, f1grad, [2.0,3.0], 0.1, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [x = i for i=1:length(iter[:,1])]\n",
    "Plots.plot(k,iter[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = [x = i for i=10:length(iter[:,1])]\n",
    "Plots.plot(k,iter[10:length(iter[:,1]),1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente par coordonnée (Coordinate descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function GaussSeidel(f::Function, x0, h::Float64, verbose::Bool = true, δ::Float64 = 1e-6, maxiter::Int64 = 1000)\n",
    "\n",
    "    function fsearch(α::Float64)\n",
    "        return(f(x+α*d))\n",
    "    end\n",
    "\n",
    "    x = copy(x0)\n",
    "    n = length(x)\n",
    "    k = 0\n",
    "    d = zeros(n)\n",
    "    \n",
    "    while true\n",
    "        x0[:] = x[:]\n",
    "        k += 1\n",
    "        \n",
    "        for i = 1:n\n",
    "            d[i] = 1.0  # d is now the i^th vector of the canonical basis\n",
    "            α = Optim.minimizer(optimize(fsearch, -h, h, GoldenSection()))\n",
    "            x[i] += α\n",
    "            d[i] = 0.0\n",
    "        end\n",
    "        \n",
    "        if verbose\n",
    "            println(k, \". \", f(x), \" \", x, \" \", x0)\n",
    "        end\n",
    "        \n",
    "        if norm(x-x0) < δ\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Jacobi(f::Function, x0, h::Float64, verbose::Bool = true, δ::Float64 = 1e-6, maxiter::Int64 = 1000)\n",
    "\n",
    "    function fsearch(α::Float64)\n",
    "        return(f(x0+α*d))\n",
    "    end\n",
    "\n",
    "    x = copy(x0)\n",
    "    n = length(x)\n",
    "    k = 0\n",
    "    d = zeros(n)\n",
    "    α = zeros(n)\n",
    "    \n",
    "    while true\n",
    "        x0[:] = x[:]\n",
    "        k += 1\n",
    "        \n",
    "        for i = 1:n\n",
    "            d[i] = 1.0  # d is now the i^th vector of the canonical basis\n",
    "            α[i] = Optim.minimizer(optimize(fsearch, -h, h, GoldenSection()))\n",
    "            d[i] = 0.0\n",
    "        end\n",
    "        x += α\n",
    "        \n",
    "        if verbose\n",
    "            println(k, \". \", f(x), \" \", x, \" \", x0)\n",
    "        end\n",
    "        \n",
    "        if norm(x-x0) < δ\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = GaussSeidel(f1, [2.0,3.0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = Jacobi(f1, [2.0,3.0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons la fonction bivariée\n",
    "$$\n",
    "f(x,y) = \\frac{(2-x)^2}{2y^2}+\\frac{(3-x)^2}{2y^2} + \\ln y\n",
    "$$\n",
    "qui est calculée de Julia comme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = (2-x[1])*(2-x[1])/(2*x[2]*x[2])+(3-x[1])*(3-x[1])/(2*x[2]*x[2])+log(x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sa dérivée est\n",
    "$$\n",
    "\\nabla f(x) =\n",
    "\\begin{pmatrix}\n",
    "\\frac{-2(2-x)}{2y^2}+\\frac{-2(3-x)}{2y^2} \\\\\n",
    "-\\frac{(2-x)^2}{y^3}-\\frac{(3-x)^2}{y^3} + \\frac{1}{y}\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\frac{x-2}{y^2}+\\frac{x-3}{y^2} \\\\\n",
    "-\\frac{(2-x)^2}{y^3}-\\frac{(3-x)^2}{y^3} + \\frac{1}{y}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fprime(x)\n",
    "    return [(x[1]-2)/(x[2]*x[2])+(x[1]-3)/(x[2]*x[2]),\n",
    "            -(2-x[1])*(2-x[1])/(x[2]*x[2]*x[2])-(3-x[1])*(3-x[1])/(x[2]*x[2]*x[2])+1/x[2]]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default(size=(600,600), fc=:heat)\n",
    "x, y = -2.5:0.1:2.5, 0.5:0.1:2.5\n",
    "z = Surface((x,y)->f([x,y]), x, y)\n",
    "surface(x,y,z, linealpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = steepestdescent(f, fprime, [1.0,1.0], 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le choix de $h$ est important. Considérons par exemple une valeur trop petite: $h = 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sol = steepestdescent(f, fprime, [1.0,1.0], 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un trop grand $h$ peut également conduire à des difficultés. Considérons par exemple $h = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = steepestdescent(f, fprime, [1.0,1.0], 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devrons nous assurer que les itérés garantissent que $y > 0$ en raison de l'opérateur logarithmique.\n",
    "\n",
    "Le choix du point de départ est également important afin de s'assurer que l'algorithme converge assez rapidement. Considérons par exemple $x_0 = (0.1, 0.1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = steepestdescent(f, fprime, [0.1,0.1], 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, prenons $x_0 = (100, 100)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = steepestdescent(f, fprime, [100.0,100.0], 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pratique, nous aurons souvent besoin de connaissances sur la fonction à optimiser afin d'être efficace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de Rosenbrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x,y) = (1-x)^2 + 100(y-x^2)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla f(x,y) =\n",
    "\\begin{pmatrix}\n",
    "-2(1-x)-400x(y-x^2) \\\\\n",
    "200(y-x^2)\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla^2 f(x,y) =\n",
    "\\begin{pmatrix}\n",
    "2 - 400(y-x^2) + 800x^2 & -400x \\\\\n",
    "-400x & 200\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "2 - 400y + 1200x^2 & -400x \\\\\n",
    "-400x & 200\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rosenbrock(x::Vector)\n",
    "  return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "end\n",
    " \n",
    "function rosenbrock_gradient(x::Vector)\n",
    "  return [-2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1],\n",
    "          200.0 * (x[2] - x[1]^2)]\n",
    "end\n",
    " \n",
    "function rosenbrock_hessian(x::Vector)\n",
    "  h = zeros(2, 2)\n",
    "  h[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\n",
    "  h[1, 2] = -400.0 * x[1]\n",
    "  h[2, 1] = -400.0 * x[1]\n",
    "  h[2, 2] = 200.0\n",
    "  return h\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default(size=(600,600))\n",
    "x, y = 0:0.01:1.0, 0:0.01:1.0\n",
    "z = Surface((x,y)->rosenbrock([x,y]), x, y)\n",
    "surface(x,y,z, linealpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Plots.contour(x,y,z, linealpha = 0.1, levels=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent(rosenbrock, rosenbrock_gradient, [0.0,0.0], 10.0, true, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e minimiseur est situé en $(1,1)$. En effet,\n",
    "$$\n",
    "\\nabla f(1,1) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "et\n",
    "$$\n",
    "\\nabla^2 f(1,1) =\n",
    "\\begin{pmatrix}\n",
    "802 & -400 \\\\ -400 & 200\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Les déterminants des mineurs principaux sont positifs comme ils valent respectivement 802 et $802\\times200-400^2= 400$, aussi la matrice hessienne est définie positif.\n",
    "\n",
    "Cependant, la méthode de plus forte pente converge très lentement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot!(iter[:,2], iter[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Minimisation exacte ou approximative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La minimisation exacte d'une fonction le long de la direction de recherche exige des hypothèses comme l'unimodalité ou la convexité, lesquelles ne sont pas nécessairement satisfaites. Il est plus pratique de miniser la fonction approximativement le long de la direction de recherche au mieux d'une marche arrière (backtracking). Ceci sera fait plus explicitement dans le bloc-notes sur le recherche linéaire.\n",
    "\n",
    "Pour les fonctions non-convexes, une première approche consiste à fixer la longueur du pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function batchdescent(f::Function, fprime::Function, x0, α::Float64, verbose::Bool = true,\n",
    "                      record::Bool = false, tol::Float64 = 1e-7, maxiter::Int64 = 1000)\n",
    "\n",
    "    function fsearch(α::Float64)\n",
    "        return(f(x-α*grad))\n",
    "    end\n",
    "\n",
    "    x = x0\n",
    "    k = 0\n",
    "\n",
    "    grad = fprime(x)\n",
    "\n",
    "    if (verbose || record)\n",
    "        fx = f(x)\n",
    "    end\n",
    "    if (verbose)\n",
    "        println(\"$k. x = $x, f($x) = $fx\")\n",
    "    end\n",
    "    if (record)\n",
    "        iterates = [ fx x' ]\n",
    "    end\n",
    "    \n",
    "    while ((k < maxiter) && (norm(grad) > tol))\n",
    "        x = x-α*grad\n",
    "        k += 1\n",
    "        grad = fprime(x)       \n",
    "        if (verbose || record)\n",
    "            fx = f(x)\n",
    "        end\n",
    "        if (verbose)\n",
    "            println(\"$k. x = $x, f($x) = $fx\")\n",
    "        end\n",
    "        if (record)\n",
    "            iterates = [ iterates; fx x' ]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if (k == maxiter)\n",
    "        println(\"WARNING: maximum number of iterations reached\")\n",
    "    end\n",
    "\n",
    "    if (record)\n",
    "        return x, iterates\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons être proche de la solution si $\\alpha$ est assez petit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = batchdescent(f1, f1grad, [2.0,3.0], 0.1, true, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais si $\\alpha$ est trop grand, cela ne fonctionne tout simplement pas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol, iter = batchdescent(f1, f1grad, [2.0,3.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si $f \\in C^1$, $f$ convexe, et $\\nabla f(\\cdot)$ est continue au send de Lipschitz, i.e. $\\exists L >0$ tel que\n",
    "$$\n",
    "\\forall x, y,\\ \\| \\nabla f(x) - \\nabla f(y) \\|_2 \\leq L \\| x - y\\|_2,\n",
    "$$\n",
    "nous pouvons retrouver la convergence en considérant une séquence décroissante de longueurs de pas $\\alpha_k > 0$ satisfaisant\n",
    "$$\n",
    "\\sum_{k = 1}^{+\\infty} \\alpha_k = +\\infty,\\qquad \\sum_{k = 1}^{+\\infty} \\alpha_k^2 < +\\infty.\n",
    "$$\n",
    "Exemple: $\\alpha_k = \\frac{\\kappa}{k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rbdescent(f::Function, fprime::Function, x0, α0::Float64, verbose::Bool = true,\n",
    "                   record::Bool = false, tol::Float64 = 1e-7, maxiter::Int64 = 1000)\n",
    "\n",
    "    function fsearch(α::Float64)\n",
    "        return(f(x-α*grad))\n",
    "    end\n",
    "\n",
    "    x = x0\n",
    "    k = 0\n",
    "    α = α0\n",
    "\n",
    "    grad = fprime(x)\n",
    "\n",
    "    if (verbose || record)\n",
    "        fx = f(x)\n",
    "    end\n",
    "    if (verbose)\n",
    "        println(\"$k. x = $x, f($x) = $fx\")\n",
    "    end\n",
    "    if (record)\n",
    "        iterates = [ fx x' ]\n",
    "    end\n",
    "    \n",
    "    while ((k < maxiter) && (norm(grad) > tol))\n",
    "        k += 1\n",
    "        α = α0/k \n",
    "        x = x-α*grad\n",
    "        grad = fprime(x)       \n",
    "        if (verbose || record)\n",
    "            fx = f(x)\n",
    "        end\n",
    "        if (verbose)\n",
    "            println(\"$k. x = $x, f($x) = $fx\", \", α = \", α)\n",
    "        end\n",
    "        if (record)\n",
    "            iterates = [ iterates; fx x' ]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if (k == maxiter)\n",
    "        println(\"WARNING: maximum number of iterations reached\")\n",
    "    end\n",
    "\n",
    "    if (record)\n",
    "        return x, iterates\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol, iter = rbdescent(f1, f1grad, [2.0,3.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol, iter = rbdescent(f1, f1grad, [10.0,10.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol, iter = rbdescent(f1, f1grad, [100.0,100.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol, iter = rbdescent(f1, f1grad, [100.0,100.0], 0.1, true, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette technique a été proposée par Robbins et Monro en 1951 dans le contexte de l'approximation stochastique, où la fonction objectif est\n",
    "$$\n",
    "f(x) = E[g(x,\\xi)]\n",
    "$$\n",
    "et à chaque itération, le prochain itéré est calculé comme\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha_k \\nabla g(x_k,\\xi_k)\n",
    "$$\n",
    "où $\\xi_k$ est tiré de la distribution de $\\xi$.\n",
    "\n",
    "Cette technique, de même que certaines extensions (mini-lots, gradient stochastique moyen, etc.) est toujours très populaire en apprentissage automatique."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
