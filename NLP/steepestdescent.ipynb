{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthode de plus forte pente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons $f \\in C^1$. Le méthode de plus forte pente consiste à calculer itérativement\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha^* \\nabla f(x^k)\n",
    "$$\n",
    "où $\\alpha^* \\in \\arg\\min_{\\alpha \\geq 0} f(x_k - \\alpha \\nabla f(x_k))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Optim [429524aa-4258-5aef-a3af-852621145aeb]\n",
      "└ @ Base loading.jl:1278\n",
      "┌ Info: For saving to png with the Plotly backend ORCA has to be installed.\n",
      "└ @ Plots C:\\Users\\bastin\\.julia\\packages\\Plots\\6RLiv\\src\\backends.jl:373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Plots.PlotlyBackend()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Optim\n",
    "using Plots\n",
    "plotly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Optim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons besoin de la librairie `LinearAlgebra` pour accéder à des méthodes comme `det`, qui calcul le déterminant d'une matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the bivariate function\n",
    "$$\n",
    "f(x, y) = 4x^2 - 4xy + 2y^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1(x) = 4x[1]*(x[1]-x[2])+2*x[2]*x[2]\n",
    "\n",
    "default(size=(600,600), fc=:heat)\n",
    "x, y = -2.5:0.1:2.5, 0.5:0.1:2.5\n",
    "z = Surface((x,y)->f1([x,y]), x, y)\n",
    "surface(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Its gradient is\n",
    "$$\n",
    "\\nabla f(x, y) = \\begin{pmatrix} 8x - 4y \\\\ 4y - 4x \\end{pmatrix}\n",
    "$$\n",
    "The Hessian is\n",
    "$$\n",
    "\\nabla f^2(x,y) =\n",
    "\\begin{pmatrix}\n",
    "8 & -4 \\\\ -4 & 4\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [8 -4; -4 4 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal minors determinants are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8\n",
    "det( A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the matrix is positive definite. We can confirm this by computing the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the gradient as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f1grad(x)\n",
    "    return [8*x[1]-4*x[2], 4*x[2]-4*x[1]]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $x_0 = (2, 3)$. Therefore $\\nabla f(x_0) = (4, 4)$.\n",
    "\n",
    "We have to minimize the univariate function\n",
    "$$\n",
    "m(\\alpha) = f((2, 3) - \\alpha(4, 4)) = f(2 - 4\\alpha, 3 - 4\\alpha)\n",
    "$$\n",
    "The derivative of $m(\\alpha)$ is\n",
    "\\begin{align*}\n",
    "m'(\\alpha) &= \\nabla_{(x,y)} f(2 - 4\\alpha, 3 - 4\\alpha)^T \\nabla_{\\alpha} \\begin{pmatrix} 2 - 4\\alpha \\\\ 3 - 4\\alpha \\end{pmatrix} \\\\\n",
    "&= \\begin{pmatrix} 8(2-4\\alpha) - 4(3-4\\alpha) & 4(3-4\\alpha) - 4(2-4\\alpha)\\end{pmatrix}\\begin{pmatrix} -4 \\\\ -4 \\end{pmatrix} \\\\\n",
    "&= -\\begin{pmatrix} 4 - 16\\alpha & 4\\end{pmatrix}\\begin{pmatrix} 4 \\\\ 4 \\end{pmatrix} \\\\\n",
    "&= -16+64\\alpha-16\\\\\n",
    "&= 64\\alpha-32\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second derivate of $m(\\alpha)$ is\n",
    "$$\n",
    "m''(\\alpha) = 64\n",
    "$$\n",
    "Therefore the unidimensionel model is strictly convex. The minimizer can be found by setting $m'(\\alpha^*) = 0$, leading to $\\alpha^* = \\frac{1}{2}$. Therefore\n",
    "$$\n",
    "x_1 = x_0 - \\frac{1}{2}\\nabla f(x_0) = (2, 3) - \\frac{1}{2}(4, 4) = (0, 1),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla f(x_1) = \\begin{pmatrix} -4 \\\\ 4 \\end{pmatrix}\n",
    "$$\n",
    "The univariate function to minimize is now\n",
    "$$\n",
    "m(\\alpha) = f((0, 1) - \\alpha(-4, 4)) = f(4\\alpha, 1 - 4\\alpha)\n",
    "$$\n",
    "and its derivative is\n",
    "\\begin{align*}\n",
    "m'(\\alpha) &= \\nabla_{(x,y)} f(4\\alpha, 1 - 4\\alpha)^T \\nabla_{\\alpha} \\begin{pmatrix} 4\\alpha \\\\ 1 - 4\\alpha \\end{pmatrix} \\\\\n",
    "&= ( 8 \\times 4\\alpha - 4(1-4\\alpha), 4(1-4\\alpha) - 4\\times(4\\alpha))\\begin{pmatrix} 4 \\\\ -4 \\end{pmatrix} \\\\\n",
    "&= ( -4 + 48\\alpha, 4 - 32 \\alpha)\\begin{pmatrix} 4 \\\\ -4 \\end{pmatrix} \\\\\n",
    "&= -32+320\\alpha\n",
    "\\end{align*}\n",
    "The root of $m'(\\alpha)$ is $\\alpha^* = \\frac{1}{10}$, and $m''(\\alpha) = 320$, thus $\\alpha^*$ is a global minimizer.\n",
    "We obtain\n",
    "$$\n",
    "x_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\frac{1}{10}\\begin{pmatrix} -4 \\\\ 4 \\end{pmatrix}\n",
    "= \\begin{pmatrix} \\frac{4}{10} \\\\ \\frac{6}{10} \\end{pmatrix}\n",
    "= \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{3}{5} \\end{pmatrix}\n",
    "$$\n",
    "We could continue, but such a hand computation is tedious. We will automatize the procedure by constructing a Julia function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function steepestdescent(f::Function, fprime::Function, x0, h::Float64, verbose::Bool = true,\n",
    "                         record::Bool = false, tol::Float64 = 1e-7, maxiter::Int64 = 1000)\n",
    "\n",
    "    function fsearch(α::Float64)\n",
    "        return(f(x-α*grad))\n",
    "    end\n",
    "\n",
    "    x = x0\n",
    "    k = 0\n",
    "\n",
    "    grad = fprime(x)\n",
    "\n",
    "    if (verbose || record)\n",
    "        fx = f(x)\n",
    "    end\n",
    "    if (verbose)\n",
    "        println(\"$k. x = $x, f($x) = $fx\")\n",
    "    end\n",
    "    if (record)\n",
    "        iterates = [ fx x' ]\n",
    "    end\n",
    "    \n",
    "    while ((k < maxiter) && (norm(grad) > tol))\n",
    "        α = Optim.minimizer(optimize(fsearch, 0, h, GoldenSection()))\n",
    "        x = x-α*grad\n",
    "        k += 1\n",
    "        grad = fprime(x)       \n",
    "        if (verbose || record)\n",
    "            fx = f(x)\n",
    "        end\n",
    "        if (verbose)\n",
    "            println(\"$k. x = $x, f($x) = $fx\")\n",
    "        end\n",
    "        if (record)\n",
    "            iterates = [ iterates; fx x' ]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if (k == maxiter)\n",
    "        println(\"WARNING: maximum number of iterations reached\")\n",
    "    end\n",
    "\n",
    "    if (record)\n",
    "        return x, iterates\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variant proposes to enlarge the interval where the unidimensional search is done when the upper bound is reached.\n",
    "\n",
    "This is only valid for convex functions!\n",
    "\n",
    "But the idea will be adapted and generalized when discussing about trust regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function steepestdescent_convex(f::Function, fprime::Function, x0, h::Float64, verbose::Bool = true,\n",
    "        record::Bool = false, tol::Float64 = 1e-7, maxiter::Int64 = 1000)\n",
    "\n",
    "    function fsearch(α::Float64)\n",
    "        return(f(x-α*grad))\n",
    "    end\n",
    "\n",
    "    x = x0\n",
    "    k = 0\n",
    "\n",
    "    grad = fprime(x)\n",
    "\n",
    "    if (verbose || record)\n",
    "        fx = f(x)\n",
    "    end\n",
    "    if (verbose)\n",
    "        println(\"$k. x = $x, f($x) = $fx\")\n",
    "    end\n",
    "    if (record)\n",
    "        iterates = [ fx x' ]\n",
    "    end\n",
    "\n",
    "    Δ = 1e-6\n",
    "    \n",
    "    while ((k < maxiter) && (norm(grad) > tol))\n",
    "        α = Optim.minimizer(optimize(fsearch, 0, h, GoldenSection()))\n",
    "        while ((h-α) <= Δ)\n",
    "            h *= 2\n",
    "            α = Optim.minimizer(optimize(fsearch, α, h, GoldenSection()))\n",
    "        end\n",
    "        h = α\n",
    "        x = x-α*grad\n",
    "        k += 1\n",
    "        grad = fprime(x)       \n",
    "        if (verbose || record)\n",
    "            fx = f(x)\n",
    "        end\n",
    "        if (verbose)\n",
    "            println(\"$k. x = $x, f($x) = $fx\")\n",
    "        end\n",
    "        if (record)\n",
    "            iterates = [ iterates; fx x' ]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if (k == maxiter)\n",
    "        println(\"WARNING: maximum number of iterations reached\")\n",
    "    end\n",
    "\n",
    "    if (record)\n",
    "        return x, iterates\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing this function on the problem, we obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent(f1, f1grad, [2.0,3.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent(f1, f1grad, [10.0,10.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent(f1, f1grad, [100.0,100.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We converge to the solution $(0,0)$, but the method was quite slow close to the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent(f1, f1grad, [2.0,3.0], 0.1, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent_convex(f1, f1grad, [2.0,3.0], 0.1, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [x = i for i=1:length(iter[:,1])]\n",
    "Plots.plot(k,iter[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = [x = i for i=10:length(iter[:,1])]\n",
    "Plots.plot(k,iter[10:length(iter[:,1]),1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Jacobi(f::Function, x0, h::Float64, verbose::Bool = true, δ::Float64 = 1e-6, maxiter::Int64 = 1000)\n",
    "\n",
    "    function fsearch(α::Float64)\n",
    "        return(f(x0-α*d))\n",
    "    end\n",
    "\n",
    "    x = copy(x0)\n",
    "    n = length(x)\n",
    "    k = 0\n",
    "    d = zeros(n)\n",
    "    \n",
    "    while true\n",
    "        x0[:] = x[:]\n",
    "        k += 1\n",
    "        \n",
    "        for i = 1:n\n",
    "            d[i] = 1.0  # d is now the i^th vector of the canonical basis\n",
    "            α = Optim.minimizer(optimize(fsearch, 0, h, GoldenSection()))\n",
    "            x[i] -= α\n",
    "            d[i] = 0.0\n",
    "        end\n",
    "        \n",
    "        if verbose\n",
    "            println(k, \". \", f(x), \" \", x, \" \", x0)\n",
    "        end\n",
    "        \n",
    "        if norm(x-x0) < δ\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = Jacobi(f1, [2.0,3.0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the bivariate function\n",
    "$$\n",
    "f(x,y) = \\frac{(2-x)^2}{2y^2}+\\frac{(3-x)^2}{2y^2} + \\ln y\n",
    "$$\n",
    "that is computed in Julia as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = (2-x[1])*(2-x[1])/(2*x[2]*x[2])+(3-x[1])*(3-x[1])/(2*x[2]*x[2])+log(x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its derivative is\n",
    "$$\n",
    "\\nabla f(x) =\n",
    "\\begin{pmatrix}\n",
    "\\frac{-2(2-x)}{2y^2}+\\frac{-2(3-x)}{2y^2} \\\\\n",
    "-\\frac{(2-x)^2}{y^3}-\\frac{(3-x)^2}{y^3} + \\frac{1}{y}\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\frac{x-2}{y^2}+\\frac{x-3}{y^2} \\\\\n",
    "-\\frac{(2-x)^2}{y^3}-\\frac{(3-x)^2}{y^3} + \\frac{1}{y}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fprime(x)\n",
    "    return [(x[1]-2)/(x[2]*x[2])+(x[1]-3)/(x[2]*x[2]),\n",
    "            -(2-x[1])*(2-x[1])/(x[2]*x[2]*x[2])-(3-x[1])*(3-x[1])/(x[2]*x[2]*x[2])+1/x[2]]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default(size=(600,600), fc=:heat)\n",
    "x, y = -2.5:0.1:2.5, 0.5:0.1:2.5\n",
    "z = Surface((x,y)->f([x,y]), x, y)\n",
    "surface(x,y,z, linealpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = steepestdescent(f, fprime, [1.0,1.0], 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of $h$ is important. Consider for instance a too small value: $h = 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sol = steepestdescent(f, fprime, [1.0,1.0], 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But a too big $h$ can lead to some issues too. Consider for instance $h = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = steepestdescent(f, fprime, [1.0,1.0], 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to ensure that the iterates are such that $y > 0$ due to the logarithmic operator.\n",
    "\n",
    "The choice of the starting point is also important to ensure that the algorithm converges fast enough. Consider for instance $x_0 = (0.1, 0.1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = steepestdescent(f, fprime, [0.1,0.1], 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take $x_0 = (100, 100)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = steepestdescent(f, fprime, [100.0,100.0], 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we often need some insight on the function to optimize in order to be efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x,y) = (1-x)^2 + 100(y-x^2)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla f(x,y) =\n",
    "\\begin{pmatrix}\n",
    "-2(1-x)-400x(y-x^2) \\\\\n",
    "200(y-x^2)\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla^2 f(x,y) =\n",
    "\\begin{pmatrix}\n",
    "2 - 400(y-x^2) + 800x^2 & -400x \\\\\n",
    "-400x & 200\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "2 - 400y + 1200x^2 & -400x \\\\\n",
    "-400x & 200\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rosenbrock(x::Vector)\n",
    "  return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "end\n",
    " \n",
    "function rosenbrock_gradient(x::Vector)\n",
    "  return [-2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1],\n",
    "          200.0 * (x[2] - x[1]^2)]\n",
    "end\n",
    " \n",
    "function rosenbrock_hessian(x::Vector)\n",
    "  h = zeros(2, 2)\n",
    "  h[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\n",
    "  h[1, 2] = -400.0 * x[1]\n",
    "  h[2, 1] = -400.0 * x[1]\n",
    "  h[2, 2] = 200.0\n",
    "  return h\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default(size=(600,600))\n",
    "x, y = 0:0.01:1.0, 0:0.01:1.0\n",
    "z = Surface((x,y)->rosenbrock([x,y]), x, y)\n",
    "surface(x,y,z, linealpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Plots.contour(x,y,z, linealpha = 0.1, levels=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = steepestdescent(rosenbrock, rosenbrock_gradient, [0.0,0.0], 10.0, true, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimizer is located at $(1,1)$. Indeed,\n",
    "$$\n",
    "\\nabla f(1,1) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla^2 f(1,1) =\n",
    "\\begin{pmatrix}\n",
    "802 & -400 \\\\ -400 & 200\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The determinants of the principal minors are positive as they are respectively 802 and $802\\times200-400^2= 400$, so the Hessian is positive definite.\n",
    "\n",
    "However the steepest descent method converges very slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot!(iter[:,2], iter[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exact minimization of approximate minimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact minimization of the function along the search direction requires assumptions as unimodality or convexity, that are not necessarily satisfied. It is more practical to approximately minimize the function along the search direction using backtracking. This will be done more explicitely in the linesearch notebook.\n",
    "\n",
    "For nonconvex functions, a first approach is to fix the step length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function batchdescent(f::Function, fprime::Function, x0, α::Float64, verbose::Bool = true,\n",
    "                      record::Bool = false, tol::Float64 = 1e-7, maxiter::Int64 = 1000)\n",
    "\n",
    "    function fsearch(α::Float64)\n",
    "        return(f(x-α*grad))\n",
    "    end\n",
    "\n",
    "    x = x0\n",
    "    k = 0\n",
    "\n",
    "    grad = fprime(x)\n",
    "\n",
    "    if (verbose || record)\n",
    "        fx = f(x)\n",
    "    end\n",
    "    if (verbose)\n",
    "        println(\"$k. x = $x, f($x) = $fx\")\n",
    "    end\n",
    "    if (record)\n",
    "        iterates = [ fx x' ]\n",
    "    end\n",
    "    \n",
    "    while ((k < maxiter) && (norm(grad) > tol))\n",
    "        x = x-α*grad\n",
    "        k += 1\n",
    "        grad = fprime(x)       \n",
    "        if (verbose || record)\n",
    "            fx = f(x)\n",
    "        end\n",
    "        if (verbose)\n",
    "            println(\"$k. x = $x, f($x) = $fx\")\n",
    "        end\n",
    "        if (record)\n",
    "            iterates = [ iterates; fx x' ]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if (k == maxiter)\n",
    "        println(\"WARNING: maximum number of iterations reached\")\n",
    "    end\n",
    "\n",
    "    if (record)\n",
    "        return x, iterates\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get close too the solution if $\\alpha$ is small enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol, iter = batchdescent(f1, f1grad, [2.0,3.0], 0.1, true, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if $\\alpha$ is too large, it does not work at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol, iter = batchdescent(f1, f1grad, [2.0,3.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $f \\in C^1$, $f$ convex, and $\\nabla f(\\cdot)$ is Lipschitz continuous, i.e. $\\exists L >0$ such that\n",
    "$$\n",
    "\\forall x, y,\\ \\| \\nabla f(x) - \\nabla f(y) \\|_2 \\leq L \\| x - y\\|_2,\n",
    "$$\n",
    "we can recover the convergence by considering a decreasing sequence of step lengths $\\alpha_k > 0$ staisfying\n",
    "$$\n",
    "\\sum_{k = 1}^{+\\infty} \\alpha_k = +\\infty,\\qquad \\sum_{k = 1}^{+\\infty} \\alpha_k^2 < +\\infty.\n",
    "$$\n",
    "Example: $\\alpha_k = \\frac{\\kappa}{k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rbdescent(f::Function, fprime::Function, x0, α0::Float64, verbose::Bool = true,\n",
    "                   record::Bool = false, tol::Float64 = 1e-7, maxiter::Int64 = 1000)\n",
    "\n",
    "    function fsearch(α::Float64)\n",
    "        return(f(x-α*grad))\n",
    "    end\n",
    "\n",
    "    x = x0\n",
    "    k = 0\n",
    "    α = α0\n",
    "\n",
    "    grad = fprime(x)\n",
    "\n",
    "    if (verbose || record)\n",
    "        fx = f(x)\n",
    "    end\n",
    "    if (verbose)\n",
    "        println(\"$k. x = $x, f($x) = $fx\")\n",
    "    end\n",
    "    if (record)\n",
    "        iterates = [ fx x' ]\n",
    "    end\n",
    "    \n",
    "    while ((k < maxiter) && (norm(grad) > tol))\n",
    "        k += 1\n",
    "        α = α0/k \n",
    "        x = x-α*grad\n",
    "        grad = fprime(x)       \n",
    "        if (verbose || record)\n",
    "            fx = f(x)\n",
    "        end\n",
    "        if (verbose)\n",
    "            println(\"$k. x = $x, f($x) = $fx\", \", α = \", α)\n",
    "        end\n",
    "        if (record)\n",
    "            iterates = [ iterates; fx x' ]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if (k == maxiter)\n",
    "        println(\"WARNING: maximum number of iterations reached\")\n",
    "    end\n",
    "\n",
    "    if (record)\n",
    "        return x, iterates\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol, iter = rbdescent(f1, f1grad, [2.0,3.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol, iter = rbdescent(f1, f1grad, [10.0,10.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol, iter = rbdescent(f1, f1grad, [100.0,100.0], 2.0, true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol, iter = rbdescent(f1, f1grad, [100.0,100.0], 0.1, true, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique has been proposed by Robbins and Monro in 1951 in the context of stochastic approximation, where the objective is\n",
    "$$\n",
    "f(x) = E[g(x,\\xi)]\n",
    "$$\n",
    "and at each iteration, the next iterate is computed as\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha_k \\nabla g(x_k,\\xi_k)\n",
    "$$\n",
    "where $\\xi_k$ is drawn from the distribution of $\\xi$.\n",
    "\n",
    "This technique, as well as some extensions (mini-batch, stochastic average gradient,...) is still very popular in machine learning."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
