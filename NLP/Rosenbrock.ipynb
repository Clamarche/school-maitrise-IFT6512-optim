{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction de Rosenbrock généralisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using ForwardDiff\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs généralisations à $n$ dimensions de la fonction de Rosenbrock ont été proposées dans la littérature. Nous considérons ici la généralisation définie:\n",
    "$$\n",
    "f(x) = \\sum_{i = 1}^{n-1} \\left( 100(x_{i+1}^2-x_i)^2 + (x_i-1)^2 \\right).\n",
    "$$\n",
    "Il s'agit de la somme de $n-1$ fonctions de Rosenbrock à 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rosenbrock(x::Vector)\n",
    "    return sum(100*(x[i+1]^2 - x[i])^2 + (x[i] - 1)^2 for i in 1:n-1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gradient est\n",
    "$$\n",
    "\\nabla_x f(x) =\n",
    "\\begin{pmatrix}\n",
    "-200(x_{(2)}^2-x_{(1)})+2(x_{(1)}-1) \\\\\n",
    "400x_{(2)}(x_{(2)}^2-x_{(1)})-200(x_{(3)}^2-x_{(2)})+2(x_{(2)}-1) \\\\\n",
    "\\vdots \\\\\n",
    "400x_{(i)}(x_{(i)}^2-x_{(i-1)})-200(x_{(i+1)}^2-x_{(i)})+2(x_{(i)}-1) \\\\\n",
    "\\vdots \\\\\n",
    "400x_{(n-1)}(x_{(n-1)}^2-x_{(n-2)})-200(x_{(n-1)}^2-x_{(n-2)})+2(x_{(n-2)}-1) \\\\\n",
    "400x_{(n)}(x_{(n)}^2-x_{(n-1)})\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ∇f(x:: Vector)\n",
    "    n = length(x)\n",
    "    g = zeros(n)\n",
    "    for i = 1:n-1\n",
    "        g[i] = -200*(x[i+1]^2-x[i])+2*(x[i]-1)\n",
    "    end\n",
    "    for i = 2:n\n",
    "        g[i] += 400*x[i]*(x[i]^2-x[i-1])\n",
    "    end\n",
    "    return g\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice hessienne est\n",
    "$$\n",
    "\\nabla_{xx}^2 f(x) =\n",
    "\\begin{pmatrix}\n",
    "202 & -400x_{(2)} & 0 & 0 & \\cdots & 0 & 0 & 0 \\\\\n",
    " -400x_{(2)} & 400(x_{(2)}^2-x(1))+800x_{(2)}^2+202 & -400x_{(3)} & 0 & \\cdots & 0 & 0 & 0 \\\\\n",
    "\\vdots & \\vdots &\\vdots &\\vdots & \\ddots & \\vdots &\\vdots &\\vdots &\\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & -400x_{(n-2)} & 400(x_{(n-1)}^2-x_{(n-2)})+800x_{(n-1)}^2 +202 & -400x_{(n-1)} \\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & 0 & -400x_{(n-1)} & 400(x_{(n)}^2-x_{(n-1)})+800x_{(n)}^2\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Hess(x:: Vector)\n",
    "    n = length(x)\n",
    "    H = zeros(n,n)\n",
    "    H[1,1] = 202\n",
    "    for i = 2:n\n",
    "        H[i,i-1] = H[i-1,i] = -400*x[i]\n",
    "        H[i,i] = 400*(x[i]^2-x[i-1])+800*x[i]^2 + 202\n",
    "    end\n",
    "    H[n,n] -= 202\n",
    "    return H\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les calculs sont complexes! Nous allons les vérifier à l'aide de la différentiation automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = x -> ForwardDiff.gradient(rosenbrock, x);\n",
    "H = x -> ForwardDiff.hessian(rosenbrock, x);\n",
    "function g!(storage::Vector, x::Vector)\n",
    "    s = g(x)\n",
    "    storage[1:length(s)] = s[1:length(s)]\n",
    "end\n",
    "function H!(storage::Matrix, x::Vector)\n",
    "    s = H(x)\n",
    "    n, m = size(s)\n",
    "    storage[1:n,1:m] = s[1:length(s)]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x = ones(n)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosenbrock(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(g(x)-∇f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(H(x)-Hess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La différentiation automatique est-elle efficace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark H(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark Hess(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'il y a avantage a utiliser l'implémentation exacte de la matrice hessienne, mais est-ce dû au caractère creux de la matrice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = Hess(x)\n",
    "SH = sparse(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme ce qui nous intéresse est le produit matrice-vecteur, regarde le gain potentiel obtenu avec le stockage creux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark SH*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark H*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plutôt que de construire la matrice dense puis de la réduire à une matrice creuse, nous pouvons directement utiliser les propriétés de la matrice pour exploiter sa structure, par exemple en exploitant le fait qu'il s'agit d'une matrice tridiagonale symétrique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function TriHess(x)\n",
    "    n = length(x)\n",
    "    d = zeros(n)\n",
    "    d[1] = 202\n",
    "    d[2:n] = [400*(x[i]^2-x[i-1])+800*x[i]^2 + 202 for i = 2:n]\n",
    "    d[n] -= 202\n",
    "    dl = [-400*x[i] for i = 2:n]\n",
    "    H = SymTridiagonal(d, dl)\n",
    "    return H\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TriHess(x)\n",
    "@benchmark T*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela permet de monter la taille du système, qui sinon serait ingérable. Prenons par exemple une dimension de un million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ones(1000000)/4\n",
    "T = TriHess(x)\n",
    "@benchmark T*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revenons à un exemple plus modeste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ones(1000)/4\n",
    "v = copy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosenbrock(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvr = x -> dot(g(x), v)\n",
    "@benchmark gvr(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gvr2(x:: Vector)\n",
    "    n = length(x)\n",
    "    t = (-200*(x[2]^2-x[1])+2*(x[1]-1))*v[1]\n",
    "    for i = 2:n-1\n",
    "        t += (-200*(x[i+1]^2-x[i])+2*(x[i]-1)+400*x[i]*(x[i]^2-x[i-1]))*v[i]\n",
    "    end\n",
    "    t += 400*x[n]*(x[n]^2-x[n-1])*v[n]\n",
    "    return t\n",
    "end\n",
    "@benchmark gvr2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hv = x -> ForwardDiff.gradient(gvr, x)\n",
    "norm(Hv(x)-TriHess(x)*v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hv2 = x -> ForwardDiff.gradient(gvr2, x)\n",
    "norm(Hv2(x)-TriHess(x)*v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparons les temps de calcul des différentes approches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark TriHess(x)*v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark Hess(x)*v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark Hv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark Hv2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'exploiter la structure de la matrice est plus efficace! Une grande part de la sous-performance vient de la nom prise en compte du caractère creux par la différentiation automatique, et on gagne à implémenter des calculs exacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion de matrices creuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la matrice hessienne est creuse, son inverse peut être dense. Il n'est dès lors pas pratique d'utiliser des méthodes basées sur l'inverse de la matrice hessienne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = TriHess(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factoriser la matrice n'implique pas le même niveau de remplissage. Dans le cas présent, il est remarquable de noter que la structure creuse est parfaitement respectée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorize(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela permet à nouveau de privilégier la résolutions de systèmes linéaires par factorisation à l'inversion de matrice, mais l'effort de calcul pour la factorisation reste significatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark factorize(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark H\\v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malheureusement, tous les problèmes ne présentent pas une structure aussi intéressante. Néanmoins, l'implémentation directe du produit matrice-vecteur permet de contourner un autre problème: le stockage des matrices en grande dimension. Reprenons le stockage dense. On a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark Hess(x)\\v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ones(100000)/4\n",
    "v = copy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark Hess(x)\\v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark TriHess(x)\\v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark Hv2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque le nombre de variables devient trop grand, la résolution directe du système linéaire avec une matrice dense est prohibitive (le code a pu résulter sur une erreur de débordement mémoire), alors qu'il est raisonnable de calculer le produit matrice-vecteur pour quelques itérations, ouvrant la voie aux méthodes de gradient conjugué tronqués. Exploiter la structure reste cependant à nouveau plus efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
