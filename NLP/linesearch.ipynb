{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthodes de recherche linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons $f \\in C^2$.  Une méthode de descente consiste à calculer itérativement\n",
    "$$\n",
    "x_{k+1} = x_k + \\alpha^* d_k\n",
    "$$\n",
    "où $\\alpha^*$ minimise approximativement $f(x_k - \\alpha d_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs techniques de recherche linéaire sont proposées en Julia, comme expliqué à la page https://github.com/JuliaNLSolvers/LineSearches.jl\n",
    "\n",
    "Considérons à nouveau l'exemple de Rosenbrock, dont l'expression mathématique est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x,y) = (1-x)^2 + 100(y-x^2)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son gradient peut être calculé comme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla f(x,y) =\n",
    "\\begin{pmatrix}\n",
    "-2(1-x)-400x(y-x^2) \\\\\n",
    "200(y-x^2)\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla^2 f(x,y) =\n",
    "\\begin{pmatrix}\n",
    "2 - 400(y-x^2) + 800x^2 & -400x \\\\\n",
    "-400x & 200\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "2 - 400y + 1200x^2 & -400x \\\\\n",
    "-400x & 200\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le minimiseur est situé en $(1,1)$. En effet,\n",
    "$$\n",
    "\\nabla f(1,1) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "et\n",
    "$$\n",
    "\\nabla^2 f(1,1) =\n",
    "\\begin{pmatrix}\n",
    "802 & -400 \\\\ -400 & 200\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Les déterminants des mineurs principaux sont positifs comme ils valent respectivement 802 et $802\\times200-400^2= 400$, aussi la matrice hessienne est définie positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenbrock function\n",
    "# Source: https://bitbucket.org/lurk3r/optim.jl\n",
    "\n",
    "function rosenbrock(x::Vector)\n",
    "    return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "end\n",
    "\n",
    "function rosenbrock_gradient!(storage::Vector, x::Vector)\n",
    "    storage[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n",
    "    storage[2] = 200.0 * (x[2] - x[1]^2)\n",
    "end\n",
    "\n",
    "function rosenbrock_hessian!(storage::Matrix, x::Vector)\n",
    "    storage[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\n",
    "    storage[1, 2] = -400.0 * x[1]\n",
    "    storage[2, 1] = -400.0 * x[1]\n",
    "    storage[2, 2] = 200.0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "default(size=(600,600), fc=:heat)\n",
    "x, y = 0:0.02:1.0, 0:0.02:1.0\n",
    "z = Surface((x,y)->rosenbrock([x,y]), x, y)\n",
    "surface(x,y,z, linealpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Plots.contour(x,y,z, linealpha = 0.1, levels=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons résoudre le problème d'optimisation avec la fonction `optimize` présente dans la librairie `Optim.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize(rosenbrock, rosenbrock_gradient!,\n",
    "               [20.0, 20.0],\n",
    "               Optim.GradientDescent(),\n",
    "               Optim.Options(g_tol = 1e-12,\n",
    "                             store_trace = true,\n",
    "                             show_trace = true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize(rosenbrock, rosenbrock_gradient!,\n",
    "               [20.0, 20.0],\n",
    "               Optim.BFGS(),\n",
    "               Optim.Options(g_tol = 1e-12,\n",
    "                             store_trace = true,\n",
    "                             show_trace = true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime res = optimize(rosenbrock, rosenbrock_gradient!,\n",
    "                   [0.0, 0.0], Optim.BFGS(),\n",
    "               Optim.Options(g_tol = 1e-12,\n",
    "                             store_trace = true,\n",
    "                             show_trace = false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = Optim.trace(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Différentiation en Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer le gradient et la matrice hessienne est souvent ardu, et même pour des fonctions simples, cela peut s'avérer une tâche pénible. Afin d'alléger ce coût, il est possible d'utiliser des dérivées numériques ou la différentiation automatique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dérivées numériques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des fonctions de dérivation numérique sont fournies dans la librairie `Calculus`, comme illustré ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Calculus, LinearAlgebra\n",
    "rg = Calculus.gradient(rosenbrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Évaluons la fonction de gradient, nouvellement construite, à la solution [1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsol = rg([1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous sommes proches de zéro, mais il y a des erreurs d'approximation, lesquelles peuvent empêcher la convergence vers une solution correcte, ou, à tout le mois, nuire à la precision de la solution, comme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(gsol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = [0.0,0.0]\n",
    "function rg!(storage::Vector, x::Vector)\n",
    "    s = rg(x)\n",
    "    storage[1:length(s)] = s[1:length(s)]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@btime res = optimize(rosenbrock, rg!,\n",
    "               [0.0, 0.0],\n",
    "               Optim.BFGS(),\n",
    "               Optim.Options(g_tol = 1e-12,\n",
    "                             store_trace = true,\n",
    "                             show_trace = false))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différentiation automatique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "g = x -> ForwardDiff.gradient(rosenbrock, x);\n",
    "H = x -> ForwardDiff.hessian(rosenbrock, x)\n",
    "\n",
    "function g!(storage::Vector, x::Vector)\n",
    "    s = g(x)\n",
    "    storage[1:length(s)] = s[1:length(s)]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g([1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize(rosenbrock, g!,\n",
    "               [0.0, 0.0],\n",
    "               Optim.BFGS(),\n",
    "               Optim.Options(g_tol = 1e-12,\n",
    "                             store_trace = true,\n",
    "                             show_trace = false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime res = optimize(rosenbrock, g!,\n",
    "               [0.0, 0.0],\n",
    "               Optim.BFGS(),\n",
    "               Optim.Options(g_tol = 1e-12,\n",
    "                             store_trace = true,\n",
    "                             show_trace = false))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_{k+1} = x_k-\\nabla^2 f(x_k)^{-1} \\nabla f(x_k)\n",
    "$$\n",
    "ou\n",
    "$$\n",
    "\\nabla^2 f(x_k) x_{k+1} = \\nabla^2 f(x_k) x_k- \\nabla f(x_k)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Newton(f::Function, g::Function, h:: Function,\n",
    "        xstart::Vector, verbose::Bool = false,\n",
    "        δ::Float64 = 1e-6, nmax::Int64 = 1000)\n",
    "\n",
    "    k = 1\n",
    "    x = xstart\n",
    "    n = length(x)\n",
    "    δ2 = δ*δ\n",
    "    H = zeros(n,n)+I\n",
    "    dfx = ones(n)\n",
    "    \n",
    "    if (verbose)\n",
    "        fx = f(x)\n",
    "        println(\"$k. x = $x, f(x) = $fx\")\n",
    "    end\n",
    "\n",
    "    g(dfx, x)\n",
    "\n",
    "    while (dot(dfx,dfx) > δ2 && k <= nmax)\n",
    "        k += 1\n",
    "        g(dfx,x)\n",
    "        h(H,x)\n",
    "        # Hs = dfx, x_{k+1} = x_k - s\n",
    "        x -= H\\dfx  # x = x - s\n",
    "        if (verbose)\n",
    "            fx = f(x)\n",
    "            println(\"$k. x = $x, f(x) = $fx \")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newton(rosenbrock, rosenbrock_gradient!, rosenbrock_hessian!, [-100.0,100.0], true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation d'un algorithme de recherche linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un squelette très basique d'implémentation de recherche linéaire suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function ls(f::Function, g::Function, h:: Function,\n",
    "        x0::Vector,\n",
    "        direction::Function, steplength::Function,\n",
    "        δ::Float64 = 1e-8, nmax::Int64 = 1000)\n",
    "\n",
    "    k = 0\n",
    "    x = x0\n",
    "    δ2 = δ*δ\n",
    "    n = length(x)\n",
    "\n",
    "    dfx = ones(n)\n",
    "\n",
    "    g(dfx, x)\n",
    "\n",
    "#    println(\"$k. $x\")\n",
    "\n",
    "    while (dot(dfx,dfx) > δ2 && k <= nmax)\n",
    "        # Compute the search direction\n",
    "        d, dfx = direction(f,g,h,x)\n",
    "        # Compute the step length along d\n",
    "        α = steplength(f,dfx,x,d)\n",
    "        # Update the iterate\n",
    "        x += α*d\n",
    "        k += 1\n",
    "#        println(\"$k. $x\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constantStep(f::Function, dfx:: Vector, x:: Vector, d::Vector) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function direction(f::Function, g:: Function, h:: Function, x::Vector)\n",
    "    n = length(x)\n",
    "    df = ones(n)\n",
    "    H = zeros(n,n)+I\n",
    "    g(df,x)\n",
    "    h(H,x)\n",
    "    return -H\\df, df\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls(rosenbrock, rosenbrock_gradient!, rosenbrock_hessian!,\n",
    "    [0.0,0.0], direction, constantStep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ArmijoStep(f::Function, dfx::Vector, x::Vector, d:: Vector,\n",
    "    αmax:: Float64 = 1.0, β:: Float64 =0.1, κ:: Float64 =0.2)\n",
    "    \n",
    "    s = β*dot(dfx,d)\n",
    "    α = αmax\n",
    "    \n",
    "    fx = f(x)\n",
    "    fxcand = f(x+α*d)\n",
    "    \n",
    "    while (fxcand >= fx+α*s)\n",
    "        α *= κ\n",
    "        fxcand = f(x+α*d)        \n",
    "    end\n",
    "    \n",
    "    return α\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime ls(rosenbrock, rosenbrock_gradient!, rosenbrock_hessian!,\n",
    "          [0.0,0.0], direction, ArmijoStep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function direction3(f::Function, g:: Function, h:: Function, x::Vector)\n",
    "    n = length(x)\n",
    "    df = ones(n)\n",
    "    H = zeros(n,n)+I\n",
    "    g(df,x)\n",
    "    h(H,x)\n",
    "    H[1,2] = H[2,1]= 0.0\n",
    "    return -H\\df, df\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls(rosenbrock, rosenbrock_gradient!, rosenbrock_hessian!,\n",
    "    [0.0,0.0], direction3, ArmijoStep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximations de matrice hessienne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise à jour BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En supposant que $B_k$ est une matrice symétrique définie positive, la mise à jour BFGS est définie comme\n",
    "$$\n",
    "B_{k+1} = B_k - \\frac{B_ks_ks_k^T B_k}{s_k^T B_k s_k} + \\frac{y_ky_k^T}{s_k^Ty_k}\n",
    "$$\n",
    "où $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ and $s_k = x_{k+1} - x_k$.\n",
    "Son implémentation de Julia est directe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function BFGSUpdate(B, y, s)\n",
    "    Bs = B*s\n",
    "    return B - (Bs*Bs')/dot(s, Bs) + (y*y')/dot(s,y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function BFGSUpdate!(B, y, s)\n",
    "    Bs = B*s\n",
    "    B[:,:] = B - (Bs*Bs')/dot(s, Bs) + (y*y')/dot(s,y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "y = [ 1.0 2 3]'\n",
    "s = [ 0.5 0.5 0.5 ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = zeros(n,n)+I\n",
    "BFGSUpdate(B, y, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFGSUpdate!(B, y, s)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est cependant souvent plus intéressant de travailler avec l'inverse de la matrice hessienne. Une dérivation technique donne\n",
    "$$\n",
    "B_{k+1}^{-1} = \\left(I - \\frac{s_ky_k^T}{s_k^T y_k} \\right) B_k^{-1} \\left( I - \\frac{y_ks_k^T}{y_k^Ts_k} \\right) + \\frac{s_k s_k^T}{y_k^Ts_k}\n",
    "$$\n",
    "L'implémentation Julia correspondante suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function InvBFGSUpdate_naive(invB::Matrix, y::Vector, s::Vector)\n",
    "    ys = dot(y, s)\n",
    "    A = I-(s*y')/ys\n",
    "    return A*invB*A' + (s*s')/ys         \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette implémentation est cependant inefficace comme elle implique une matrice temporaire. Nous pourrions l'éviter en réorganisant les termes, considérant que $B_{k}^{-1}$ est symétrique, et que $y_{k}^{T}B_k^{-1}y_k$ ainsi que $s_k^Ty_k$ sont des scalaires. Ceci conduit à\n",
    "$$\n",
    "B_{k+1}^{-1} = B_{k}^{-1} + \\frac{(s_k^Ty_k+y_k^TB_{k}^{-1}y_k)s_ks_k^T}{(s_k^Ty_k)^2} -\n",
    "\\frac{B_k^{-1}y_ks_k^T + s_ky_k^TB_k^{-1}}{s_k^Ty_k}\n",
    "$$\n",
    "L'implémentation Julia correspondante est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function InvBFGSUpdate(invB::Matrix, y::Vector, s::Vector)\n",
    "    ys = dot(y, s)\n",
    "    invBy = invB*y\n",
    "    return invB+1.0/ys*((ys+dot(y, invBy))/ys*(s*s') - invBy*s' - s*invBy')\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "B = invB = zeros(n,n)+I\n",
    "n, m = size(B)\n",
    "\n",
    "s = [-1.75,-0.75]\n",
    "y = [-8.5,-5.0]\n",
    "\n",
    "Bp = BFGSUpdate(B, y, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(Bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invBp = InvBFGSUpdate_naive(invB, y, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invBp = InvBFGSUpdate(invB, y, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime InvBFGSUpdate_naive(invB, y, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime InvBFGSUpdate(invB, y, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "B = invB = zeros(n,n)+I\n",
    "n, m = size(B)\n",
    "s = rand(n)\n",
    "y = 10 * rand(n)\n",
    "\n",
    "A1 = InvBFGSUpdate_naive(invB, y, s)\n",
    "\n",
    "@btime InvBFGSUpdate_naive(invB, y, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = InvBFGSUpdate(invB, y, s)\n",
    "\n",
    "@btime InvBFGSUpdate(invB, y, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(A1-A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise à jour SR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapté de https://en.wikipedia.org/wiki/Symmetric_rank-one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "B_{k+1} = B_{k} + {\\frac {(y_{k}-B_{k}s_{k})(y_{k}-B_{k}s_{k})^{T}}{(y_{k}-B_{k}s_{k})^{T}s_k}},\n",
    "$$\n",
    "où $y_k = \\nabla f(x_{\\rm cand}) - \\nabla f(x_k)$ et $s_k = x_{\\rm cand} - x_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici $B_k$ n'est pas nécessairement définie positive.\n",
    "\n",
    "Le mise à jour correspondante de l'approximation  de l'inverse de la matrice hessienne, $H_{k}=B_{k}^{-1}$, est\n",
    "$$\n",
    "H_{k+1}=H_{k}+{\\frac{(s_{k}-H_{k}y_{k})(s_{k}-H_{k}y_{k})^{T}}{(s_{k}-H_{k}y_{k})^{T}y_{k}}}.\n",
    "$$\n",
    "La formule SR1 a été redécouverte à diverses reprises. Un inconvénient est que le dénominateur peut s'annuler ou être réduit à une quantité négligeable. Certains auteurs suggèrent que la mise à jour ne soit appliqués que si\n",
    "$$\n",
    "|s_{k}^{T}(y_{k}-B_{k}s_{k})|\\geq r\\|s_{k}\\|\\|y_{k}-B_{k}s_{k}\\|,\n",
    "$$\n",
    "où $r \\in (0,1)$ est un nombre petit, e.g. $10^{-8}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
